{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1619fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch # noqa\n",
    "from chromadb.config import Settings  # noqa\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import PyPDF2\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from config import Config # noqa\n",
    "from vector_store import VectorStore\n",
    "import pdfplumber\n",
    "import logging\n",
    "import re\n",
    "import spacy\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a84c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress the specific warning from pdfminer.pdfpage\n",
    "logging.getLogger(\"pdfminer.pdfpage\").setLevel(logging.ERROR)\n",
    "# You might also need to suppress pdfminer.layout in some cases\n",
    "logging.getLogger(\"pdfminer.layout\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96811e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    content: str\n",
    "    start_pos: int\n",
    "    end_pos: int\n",
    "    chunk_type: str  # 'sentence', 'paragraph', 'section'\n",
    "    section: Optional[str] = None\n",
    "    metadata: dict = None\n",
    "\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "        # Load spaCy model for sentence segmentation\n",
    "        # Customize spaCy pipeline for efficiency\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"ner\"])\n",
    "        self.nlp.enable_pipe(\"senter\")  # Modern sentence boundary detection\n",
    "\n",
    "        # Load embedding model for semantic similarity\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        # Academic paper section patterns\n",
    "        self.section_patterns = [\n",
    "            r'^(abstract|introduction|background|related work|methodology|methods|approach|model|architecture|experiments|results|evaluation|discussion|conclusion|references|acknowledgments?)\\b',\n",
    "            r'^\\d+\\.?\\s+(introduction|background|methodology|results|discussion|conclusion)',\n",
    "            r'^[IVX]+\\.?\\s+(introduction|background|methodology|results|discussion|conclusion)'\n",
    "        ]\n",
    "        \n",
    "        # Configuration\n",
    "        self.min_chunk_size = 100\n",
    "        self.max_chunk_size = 1000\n",
    "        self.overlap_sentences = 2\n",
    "        self.similarity_threshold = 0.7\n",
    "    \n",
    "    def detect_sections(self, text: str) -> list[tuple[str, int, int]]:\n",
    "        \"\"\"Detect academic paper sections and their boundaries.\"\"\"\n",
    "        sections = []\n",
    "        lines = text.split('\\n')\n",
    "        current_section = \"introduction\"  # default\n",
    "        section_start = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line_clean = line.strip().lower()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line_clean:\n",
    "                continue\n",
    "                \n",
    "            # Check if line matches section pattern\n",
    "            for pattern in self.section_patterns:\n",
    "                if re.match(pattern, line_clean):\n",
    "                    # Close previous section\n",
    "                    if sections or current_section != \"introduction\":\n",
    "                        section_end = sum(len(lines[j]) + 1 for j in range(section_start, i))\n",
    "                        sections.append((current_section, \n",
    "                                       sum(len(lines[j]) + 1 for j in range(section_start)), \n",
    "                                       section_end))\n",
    "                    \n",
    "                    # Start new section\n",
    "                    current_section = re.match(pattern, line_clean).group(1)\n",
    "                    section_start = i\n",
    "                    break\n",
    "        \n",
    "        # Add final section\n",
    "        if current_section:\n",
    "            sections.append((current_section, \n",
    "                           sum(len(lines[j]) + 1 for j in range(section_start)), \n",
    "                           len(text)))\n",
    "        \n",
    "        return sections if sections else [(\"content\", 0, len(text))]\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> list[tuple[str, int, int]]:\n",
    "        \"\"\"Split text into sentences with position tracking.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            # Clean sentence text\n",
    "            sent_text = sent.text.strip()\n",
    "            if len(sent_text) > 20:  # Filter out very short sentences\n",
    "                sentences.append((sent_text, sent.start_char, sent.end_char))\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def split_into_paragraphs(self, text: str) -> list[tuple[str, int, int]]:\n",
    "        \"\"\"Split text into paragraphs.\"\"\"\n",
    "        paragraphs = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        # Split by double newlines (paragraph breaks)\n",
    "        para_texts = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        for para_text in para_texts:\n",
    "            para_text = para_text.strip()\n",
    "            if len(para_text) > 50:  # Filter short paragraphs\n",
    "                start_pos = text.find(para_text, current_pos)\n",
    "                if start_pos != -1:\n",
    "                    end_pos = start_pos + len(para_text)\n",
    "                    paragraphs.append((para_text, start_pos, end_pos))\n",
    "                    current_pos = end_pos\n",
    "        \n",
    "        return paragraphs\n",
    "    \n",
    "    def calculate_semantic_similarity(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"Calculate semantic similarity matrix for texts.\"\"\"\n",
    "        if len(texts) < 2:\n",
    "            return np.array([[1.0]])\n",
    "        \n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def merge_similar_chunks(self, chunks: list[Chunk]) -> list[Chunk]:\n",
    "        \"\"\"Merge semantically similar adjacent chunks.\"\"\"\n",
    "        if len(chunks) <= 1:\n",
    "            return chunks\n",
    "        \n",
    "        # Get chunk texts for similarity calculation\n",
    "        chunk_texts = [chunk.content for chunk in chunks]\n",
    "        similarity_matrix = self.calculate_semantic_similarity(chunk_texts)\n",
    "        \n",
    "        merged_chunks = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(chunks):\n",
    "            current_chunk = chunks[i]\n",
    "            \n",
    "            # Check if we can merge with next chunk\n",
    "            if (i + 1 < len(chunks) and \n",
    "                len(current_chunk.content) < self.max_chunk_size and\n",
    "                similarity_matrix[i][i + 1] > self.similarity_threshold):\n",
    "                \n",
    "                next_chunk = chunks[i + 1]\n",
    "                \n",
    "                # Merge chunks\n",
    "                merged_content = current_chunk.content + \"\\n\\n\" + next_chunk.content\n",
    "                if len(merged_content) <= self.max_chunk_size:\n",
    "                    merged_chunk = Chunk(\n",
    "                        content=merged_content,\n",
    "                        start_pos=current_chunk.start_pos,\n",
    "                        end_pos=next_chunk.end_pos,\n",
    "                        chunk_type=\"merged_paragraph\",\n",
    "                        section=current_chunk.section,\n",
    "                        metadata={\n",
    "                            \"merged_from\": [current_chunk.chunk_type, next_chunk.chunk_type],\n",
    "                            \"semantic_similarity\": similarity_matrix[i][i + 1]\n",
    "                        }\n",
    "                    )\n",
    "                    merged_chunks.append(merged_chunk)\n",
    "                    i += 2  # Skip next chunk since it's merged\n",
    "                    continue\n",
    "            \n",
    "            merged_chunks.append(current_chunk)\n",
    "            i += 1\n",
    "        \n",
    "        return merged_chunks\n",
    "    \n",
    "    def create_sliding_window_chunks(self,\n",
    "                                    sentences: list[tuple[str, int, int]],\n",
    "                                    section: str) -> list[Chunk]:\n",
    "        \"\"\"Create overlapping chunks with semantic awareness.\"\"\"\n",
    "        chunks = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(sentences):\n",
    "            chunk_sentences = []\n",
    "            chunk_length = 0\n",
    "            start_idx = i\n",
    "            \n",
    "            # Build chunk up to max size\n",
    "            while (i < len(sentences) and \n",
    "                   chunk_length + len(sentences[i][0]) < self.max_chunk_size):\n",
    "                chunk_sentences.append(sentences[i])\n",
    "                chunk_length += len(sentences[i][0])\n",
    "                i += 1\n",
    "            \n",
    "            if chunk_sentences:\n",
    "                chunk_content = \" \".join([sent[0] for sent in chunk_sentences])\n",
    "                \n",
    "                chunk = Chunk(\n",
    "                    content=chunk_content,\n",
    "                    start_pos=chunk_sentences[0][1],\n",
    "                    end_pos=chunk_sentences[-1][2],\n",
    "                    chunk_type=\"sliding_window\",\n",
    "                    section=section,\n",
    "                    metadata={\n",
    "                        \"sentence_count\": len(chunk_sentences),\n",
    "                        \"start_sentence_idx\": start_idx,\n",
    "                        \"end_sentence_idx\": i - 1\n",
    "                    }\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                # Move back for overlap (but ensure progress)\n",
    "                overlap_back = min(self.overlap_sentences, len(chunk_sentences) - 1, i - start_idx - 1)\n",
    "                i = max(start_idx + 1, i - overlap_back)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_text_semantically(self, text: str, source: str) -> list[dict]:\n",
    "        \"\"\"Main semantic chunking method.\"\"\"\n",
    "        # Step 1: Detect sections\n",
    "        sections = self.detect_sections(text)\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for section_name, section_start, section_end in sections:\n",
    "            section_text = text[section_start:section_end]\n",
    "            \n",
    "            # Step 2: Split into paragraphs\n",
    "            paragraphs = self.split_into_paragraphs(section_text)\n",
    "            \n",
    "            # Step 3: Create paragraph-level chunks\n",
    "            paragraph_chunks = []\n",
    "            for para_text, para_start, para_end in paragraphs:\n",
    "                if len(para_text) >= self.min_chunk_size:\n",
    "                    chunk = Chunk(\n",
    "                        content=para_text,\n",
    "                        start_pos=section_start + para_start,\n",
    "                        end_pos=section_start + para_end,\n",
    "                        chunk_type=\"paragraph\",\n",
    "                        section=section_name,\n",
    "                        metadata={\"paragraph_length\": len(para_text)}\n",
    "                    )\n",
    "                    paragraph_chunks.append(chunk)\n",
    "                elif len(para_text) > 20:  # Small paragraphs - will be merged\n",
    "                    chunk = Chunk(\n",
    "                        content=para_text,\n",
    "                        start_pos=section_start + para_start,\n",
    "                        end_pos=section_start + para_end,\n",
    "                        chunk_type=\"small_paragraph\",\n",
    "                        section=section_name,\n",
    "                        metadata={\"paragraph_length\": len(para_text)}\n",
    "                    )\n",
    "                    paragraph_chunks.append(chunk)\n",
    "            \n",
    "            # Step 4: Merge small paragraphs\n",
    "            paragraph_chunks = self.merge_similar_chunks(paragraph_chunks)\n",
    "            \n",
    "            # Step 5: Handle large paragraphs with sliding window\n",
    "            section_chunks = []\n",
    "            for chunk in paragraph_chunks:\n",
    "                if len(chunk.content) > self.max_chunk_size:\n",
    "                    # Split large paragraph into sentences and use sliding window\n",
    "                    sentences = self.split_into_sentences(chunk.content)\n",
    "                    sliding_chunks = self.create_sliding_window_chunks(sentences, section_name)\n",
    "                    section_chunks.extend(sliding_chunks)\n",
    "                else:\n",
    "                    section_chunks.append(chunk)\n",
    "            \n",
    "            all_chunks.extend(section_chunks)\n",
    "        \n",
    "        # Step 6: Create hierarchical chunks (section-level)\n",
    "        hierarchical_chunks = []\n",
    "        for section_name, section_start, section_end in sections:\n",
    "            section_text = text[section_start:section_end]\n",
    "            if len(section_text) > self.min_chunk_size:\n",
    "                section_chunk = Chunk(\n",
    "                    content=section_text,\n",
    "                    start_pos=section_start,\n",
    "                    end_pos=section_end,\n",
    "                    chunk_type=\"section\",\n",
    "                    section=section_name,\n",
    "                    metadata={\n",
    "                        \"section_length\": len(section_text),\n",
    "                        \"is_hierarchical\": True\n",
    "                    }\n",
    "                )\n",
    "                hierarchical_chunks.append(section_chunk)\n",
    "        \n",
    "        # Combine all chunks\n",
    "        all_chunks.extend(hierarchical_chunks)\n",
    "        \n",
    "        # Step 7: Convert to output format\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(all_chunks):\n",
    "            documents.append({\n",
    "                \"content\": chunk.content,\n",
    "                \"metadata\": {\n",
    "                    \"source\": source,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"chunk_type\": chunk.chunk_type,\n",
    "                    \"section\": chunk.section,\n",
    "                    \"start_pos\": chunk.start_pos,\n",
    "                    \"end_pos\": chunk.end_pos,\n",
    "                    \"total_chunks\": len(all_chunks),\n",
    "                    **(chunk.metadata or {})\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return documents\n",
    "\n",
    "\n",
    "# Example usage and configuration\n",
    "class Config:\n",
    "    # Semantic chunking configuration\n",
    "    MIN_CHUNK_SIZE = 100\n",
    "    MAX_CHUNK_SIZE = 800\n",
    "    OVERLAP_SENTENCES = 2\n",
    "    SIMILARITY_THRESHOLD = 0.7\n",
    "    \n",
    "    # Embedding model for semantic analysis\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # or \"allenai-specter\" for scientific papers\n",
    "    \n",
    "    # Original config\n",
    "    CHROMA_PERSIST_DIRECTORY = \"./chroma_db\"\n",
    "    TOP_K_RETRIEVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c26f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document processor with semantic chunking.\"\"\"\n",
    "        self.semantic_chunker = SemanticChunker()\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using pdfplumber.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text(x_tolerance=2, use_text_flow=True)\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Complete PDF processing pipeline with semantic chunking.\n",
    "        \"\"\"\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        print(f\"Processing {filename} with semantic chunking...\")\n",
    "        \n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            print(f\"No text extracted from {filename}.\")\n",
    "            return []\n",
    "        \n",
    "        # Use semantic chunking instead of fixed-size chunking\n",
    "        documents = self.semantic_chunker.chunk_text_semantically(text, filename)\n",
    "        \n",
    "        print(f\"Processed {filename} into {len(documents)} semantic chunks.\")\n",
    "        print(f\"Chunk types: {set(doc['metadata']['chunk_type'] for doc in documents)}\")\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d220cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        # Initialize Chroma client\n",
    "        self.client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_DIRECTORY)\n",
    "\n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)\n",
    "\n",
    "        # Get or create collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=\"academic_papers\",\n",
    "            metadata={\"description\": \"Academic research papers collection\"}\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: list[dict]) -> None:\n",
    "        \"\"\"Add documents to vector store\"\"\"\n",
    "        texts = [doc[\"content\"] for doc in documents]\n",
    "        metadatas = [doc[\"metadata\"] for doc in documents]\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.encode(texts).tolist()\n",
    "\n",
    "        # Generate IDs\n",
    "        ids = [f\"{doc['metadata']['source']}_{doc['metadata']['chunk_id']}\" \n",
    "               for doc in documents]\n",
    "\n",
    "        # Add to collection\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "    def search(self, query: str, top_k: int = Config.TOP_K_RETRIEVAL) -> list[dict]:\n",
    "        \"\"\"Search for relevant documents\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query]).tolist()\n",
    "\n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        if results[\"documents\"]:\n",
    "            for i in range(len(results[\"documents\"][0])):\n",
    "                formatted_results.append({\n",
    "                    \"content\": results[\"documents\"][0][i],\n",
    "                    \"metadata\": results[\"metadatas\"][0][i],\n",
    "                    \"similarity_score\": 1 - results[\"distances\"][0][i]  # Convert distance to similarity\n",
    "                })\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    def get_collection_stats(self) -> dict:\n",
    "        \"\"\"Get statistics about the collection\"\"\"\n",
    "        count = self.collection.count()\n",
    "        return {\n",
    "            \"total_documents\": count,\n",
    "            \"embedding_model\": Config.EMBEDDING_MODEL\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd100a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        self.vector_store = VectorStore()\n",
    "        self.PROMPT_TEMPLATE = \"\"\"\n",
    "        You're an AI research assistant analyzing academic papers. Use the context to answer scientifically.\n",
    "\n",
    "        **STRUCTURED CONTEXT:**\n",
    "        {context}\n",
    "\n",
    "        **ANALYSIS TASK:**\n",
    "        1. Identify key claims relevant to: \"{query}\"\n",
    "        2. Cross-reference across sources\n",
    "        3. Generate evidence-based response\n",
    "        4. Cite sources [1-3] where applicable\n",
    "\n",
    "        **RESPONSE:**\n",
    "        \"\"\"\n",
    "        # Initialize free local model\n",
    "        print(\"Loading language model... (this may take a few minutes first time)\")\n",
    "\n",
    "        # Use a smaller, efficient model that runs locally\n",
    "        model_name =  \"google/flan-t5-base\" # \"google/flan-t5-base\" \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)\n",
    "\n",
    "            # Set up the pipeline\n",
    "            self.generator = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_length=512,\n",
    "                temperature=0.3,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            print(\"✅ Language model loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model: {e}\")\n",
    "            # Fallback to a simpler approach\n",
    "            self.generator = None\n",
    "\n",
    "    def format_context(docs: list[dict]) -> str:\n",
    "        context_lines = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            metadata = doc['metadata']\n",
    "            \n",
    "            # PRESERVE STRUCTURE\n",
    "            header = f\"SOURCE {i+1} | {metadata['source']}\"\n",
    "            section = f\"SECTION: {metadata['section']}\" if 'section' in metadata else \"\"\n",
    "            chunk_type = f\"CHUNK TYPE: {metadata['chunk_type']}\"\n",
    "            \n",
    "            # SMART TRUNCATION (preserve sentence boundaries)\n",
    "            content = doc['content']\n",
    "            if len(content) > 400:\n",
    "                last_period = content[:400].rfind('.')\n",
    "                content = content[:last_period+1] + \" [...]\"\n",
    "            \n",
    "            context_lines.append(f\"{header}\\n{section}\\n{chunk_type}\\n{content}\\n\")\n",
    "        \n",
    "        return \"\\n\\n\".join(context_lines)\n",
    "\n",
    "\n",
    "    def postprocess(self, raw_response, context_docs):\n",
    "        # Extract answer\n",
    "        answer = raw_response[0]['generated_text'].split(\"RESPONSE:\")[-1].strip()\n",
    "        \n",
    "        # Extract citations\n",
    "        source_ids = set()\n",
    "        for i in range(len(context_docs)):\n",
    "            if f\"[{i+1}]\" in answer:\n",
    "                source_ids.add(i)\n",
    "        \n",
    "        # Verify citations\n",
    "        valid_sources = [d['metadata']['source'] for i,d in enumerate(context_docs) if i in source_ids]\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": valid_sources,\n",
    "            \"context_used\": len(context_docs)\n",
    "        }\n",
    "\n",
    "\n",
    "    def generate_response(self, query: str, context_docs: list[dict]) -> dict:\n",
    "        \"\"\"Generate response using retrieved context\"\"\"\n",
    "\n",
    "        if not self.generator:\n",
    "            return {\n",
    "                \"answer\": \"Language model not available. Please check your setup.\",\n",
    "                \"sources\": [],\n",
    "                \"context_used\": 0\n",
    "            }\n",
    "\n",
    "        # Prepare context (keep it shorter for local models)\n",
    "        # STRUCTURED context formatting\n",
    "        context = self.format_context(context_docs[:3])  # Use top 3 most relevant\n",
    "\n",
    "        # DYNAMIC prompt construction\n",
    "        prompt = self.PROMPT_TEMPLATE.format(context=context, query=query)\n",
    "\n",
    "        try:\n",
    "            # GENERATE with precision control\n",
    "            response = self.generator(\n",
    "                prompt,\n",
    "                max_new_tokens=350,\n",
    "                temperature=0.2,  # Lower for factual accuracy\n",
    "                repetition_penalty=1.2,\n",
    "                num_beams=5,      # Better than greedy search\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "            # Extract the generated text\n",
    "            generated_text = response[0]['generated_text']\n",
    "\n",
    "            # Get just the answer part (after \"Answer:\")\n",
    "            if \"Answer:\" in generated_text:\n",
    "                answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                answer = generated_text[len(prompt):].strip()\n",
    "\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": [doc['metadata']['source'] for doc in context_docs],\n",
    "                \"context_used\": len(context_docs)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error generating response: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"context_used\": 0\n",
    "            }\n",
    "\n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Complete RAG pipeline\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.vector_store.search(question)\n",
    "\n",
    "        if not relevant_docs:\n",
    "            return {\n",
    "                \"answer\": \"No relevant documents found in the database.\",\n",
    "                \"sources\": [],\n",
    "                \"context_used\": 0\n",
    "            }\n",
    "\n",
    "        # Generate response\n",
    "        response = self.generate_response(question, relevant_docs)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c0b0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SemanticDocumentProcessor()\n",
    "\n",
    "file_path = \"/Users/dominikklingshirn/Projects/academic_rag/data/raw/BERT_pre-training_deep_bidirectional_transformers.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7626e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BERT_pre-training_deep_bidirectional_transformers.pdf with semantic chunking...\n",
      "Processed BERT_pre-training_deep_bidirectional_transformers.pdf into 25 semantic chunks.\n",
      "Chunk types: {'sliding_window', 'paragraph', 'section'}\n"
     ]
    }
   ],
   "source": [
    "docs = processor.process_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5583ca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT, which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be fine-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "specific architecture modifications.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute impro\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 0\n",
      "chunk_type: paragraph\n",
      "section: abstract\n",
      "start_pos: 213\n",
      "end_pos: 1094\n",
      "total_chunks: 25\n",
      "paragraph_length: 881\n",
      "\n",
      " -------------------- \n",
      "\n",
      "1 Introduction\n",
      "Language model pre-training has been shown to\n",
      "be effective for improving many natural language\n",
      "processing tasks (Dai and Le, 2015; Peters et al.,\n",
      "2018a; Radford et al., 2018; Howard and Ruder,\n",
      "2018). These include sentence-level tasks such as\n",
      "natural language inference (Bowman et al., 2015;\n",
      "Williams et al., 2018) and paraphrasing (Dolan\n",
      "and Brockett, 2005), which aim to predict the re-\n",
      "lationships between sentences by analyzing them\n",
      "holistically, as well as token-level tasks such as\n",
      "named entity recognition and question answering,\n",
      "where models are required to produce fine-grained\n",
      "output at the token level (Tjong Kim Sang and\n",
      "De Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-\n",
      "ing pre-trained language representations to down-\n",
      "stream tasks: feature-based and fine-tuning. The\n",
      "feature-based approach, such as ELMo (Peters\n",
      "et al., 2018a), uses task-specific architectures that\n",
      "include the pre-trained representations as addi-\n",
      "tional features.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 1\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 0\n",
      "end_pos: 999\n",
      "total_chunks: 25\n",
      "sentence_count: 4\n",
      "start_sentence_idx: 0\n",
      "end_sentence_idx: 3\n",
      "\n",
      " -------------------- \n",
      "\n",
      "There are two existing strategies for apply-\n",
      "ing pre-trained language representations to down-\n",
      "stream tasks: feature-based and fine-tuning. The\n",
      "feature-based approach, such as ELMo (Peters\n",
      "et al., 2018a), uses task-specific architectures that\n",
      "include the pre-trained representations as addi-\n",
      "tional features. The fine-tuning approach, such as\n",
      "the Generative Pre-trained Transformer (OpenAI\n",
      "GPT) (Radford et al., 2018), introduces minimal\n",
      "task-specific parameters, and is trained on the\n",
      "downstream tasks by simply fine-tuning all pre-\n",
      "trained parameters. The two approaches share the\n",
      "same objective function during pre-training, where\n",
      "they use unidirectional language models to learn\n",
      "general language representations. We argue that current techniques restrict the\n",
      "power of the pre-trained representations, espe-\n",
      "cially for the fine-tuning approaches.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 2\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 691\n",
      "end_pos: 1540\n",
      "total_chunks: 25\n",
      "sentence_count: 5\n",
      "start_sentence_idx: 2\n",
      "end_sentence_idx: 6\n",
      "\n",
      " -------------------- \n",
      "\n",
      "The two approaches share the\n",
      "same objective function during pre-training, where\n",
      "they use unidirectional language models to learn\n",
      "general language representations. We argue that current techniques restrict the\n",
      "power of the pre-trained representations, espe-\n",
      "cially for the fine-tuning approaches. The ma-\n",
      "jor limitation is that standard language models are\n",
      "unidirectional, and this limits the choice of archi-\n",
      "tectures that can be used during pre-training. For\n",
      "example, in OpenAI GPT, the authors use a left-to-\n",
      "right architecture, where every token can only at-\n",
      "tend to previous tokens in the self-attention layers\n",
      "of the Transformer (Vaswani et al., 2017). Such re-\n",
      "strictions are sub-optimal for sentence-level tasks,\n",
      "and could be very harmful when applying fine-\n",
      "tuning based approaches to token-level tasks such\n",
      "as question answering, where it is crucial to incor-\n",
      "porate context from both directions. In this paper, we improve the fine-tuning based\n",
      "approaches by proposing BERT:\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 3\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 1245\n",
      "end_pos: 2228\n",
      "total_chunks: 25\n",
      "sentence_count: 6\n",
      "start_sentence_idx: 5\n",
      "end_sentence_idx: 10\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Such re-\n",
      "strictions are sub-optimal for sentence-level tasks,\n",
      "and could be very harmful when applying fine-\n",
      "tuning based approaches to token-level tasks such\n",
      "as question answering, where it is crucial to incor-\n",
      "porate context from both directions. In this paper, we improve the fine-tuning based\n",
      "approaches by proposing BERT: Bidirectional\n",
      "Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi-\n",
      "rectionality constraint by using a “masked lan-\n",
      "guage model” (MLM) pre-training objective, in-\n",
      "spired by the Cloze task (Taylor, 1953). The\n",
      "masked language model randomly masks some of\n",
      "the tokens from the input, and the objective is to\n",
      "predict the original vocabulary id of the masked\n",
      "a\n",
      "r\n",
      "X\n",
      "i\n",
      "v\n",
      ":\n",
      "1\n",
      "8\n",
      "1\n",
      "0\n",
      ". 0\n",
      "4\n",
      "8\n",
      "0\n",
      "5\n",
      "v\n",
      "2\n",
      "[\n",
      "c\n",
      "s\n",
      ". C\n",
      "L\n",
      "]\n",
      "2\n",
      "4\n",
      "M\n",
      "a\n",
      "y\n",
      "2\n",
      "0\n",
      "1\n",
      "9\n",
      "word based only on its context.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 4\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 1903\n",
      "end_pos: 2718\n",
      "total_chunks: 25\n",
      "sentence_count: 7\n",
      "start_sentence_idx: 9\n",
      "end_sentence_idx: 15\n",
      "\n",
      " -------------------- \n",
      "\n",
      "0\n",
      "4\n",
      "8\n",
      "0\n",
      "5\n",
      "v\n",
      "2\n",
      "[\n",
      "c\n",
      "s\n",
      ". C\n",
      "L\n",
      "]\n",
      "2\n",
      "4\n",
      "M\n",
      "a\n",
      "y\n",
      "2\n",
      "0\n",
      "1\n",
      "9\n",
      "word based only on its context. Unlike left-to-\n",
      "right language model pre-training, the MLM ob-\n",
      "jective enables the representation to fuse the left\n",
      "and the right context, which allows us to pre-\n",
      "train a deep bidirectional Transformer. In addi-\n",
      "tion to the masked language model, we also use\n",
      "a “next sentence prediction” task that jointly pre-\n",
      "trains text-pair representations. The contributions\n",
      "of our paper are as follows:\n",
      "• We demonstrate the importance of bidirectional\n",
      "pre-training for language representations. Un-\n",
      "like Radford et al. (2018), which uses unidirec-\n",
      "tional language models for pre-training, BERT\n",
      "uses masked language models to enable pre-\n",
      "trained deep bidirectional representations. This\n",
      "is also in contrast to Peters et al. (2018a), which\n",
      "uses a shallow concatenation of independently\n",
      "trained left-to-right and right-to-left LMs.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 5\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 2641\n",
      "end_pos: 3536\n",
      "total_chunks: 25\n",
      "sentence_count: 8\n",
      "start_sentence_idx: 14\n",
      "end_sentence_idx: 21\n",
      "\n",
      " -------------------- \n",
      "\n",
      "(2018), which uses unidirec-\n",
      "tional language models for pre-training, BERT\n",
      "uses masked language models to enable pre-\n",
      "trained deep bidirectional representations. This\n",
      "is also in contrast to Peters et al. (2018a), which\n",
      "uses a shallow concatenation of independently\n",
      "trained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce\n",
      "the need for many heavily-engineered task-\n",
      "specific architectures. BERT is the first fine-\n",
      "tuning based representation model that achieves\n",
      "state-of-the-art performance on a large suite\n",
      "of sentence-level and token-level tasks, outper-\n",
      "forming many task-specific architectures. • BERT advances the state of the art for eleven\n",
      "NLP tasks. The code and pre-trained mod-\n",
      "els are available at https://github.com/\n",
      "google-research/bert. 2 Related Work\n",
      "There is a long history of pre-training general lan-\n",
      "guage representations, and we briefly review the\n",
      "most widely-used approaches in this section.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 6\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 3226\n",
      "end_pos: 4176\n",
      "total_chunks: 25\n",
      "sentence_count: 7\n",
      "start_sentence_idx: 20\n",
      "end_sentence_idx: 26\n",
      "\n",
      " -------------------- \n",
      "\n",
      "The code and pre-trained mod-\n",
      "els are available at https://github.com/\n",
      "google-research/bert. 2 Related Work\n",
      "There is a long history of pre-training general lan-\n",
      "guage representations, and we briefly review the\n",
      "most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches\n",
      "Learning widely applicable representations of\n",
      "words has been an active area of research for\n",
      "decades, including non-neural (Brown et al., 1992;\n",
      "Ando and Zhang, 2005; Blitzer et al., 2006) and\n",
      "neural (Mikolov et al., 2013; Pennington et al.,\n",
      "2014) methods. Pre-trained word embeddings\n",
      "are an integral part of modern NLP systems, of-\n",
      "fering significant improvements over embeddings\n",
      "learned from scratch (Turian et al., 2010). To pre-\n",
      "train word embedding vectors, left-to-right lan-\n",
      "guage modeling objectives have been used (Mnih\n",
      "and Hinton, 2009), as well as objectives to dis-\n",
      "criminate correct from incorrect words in left and\n",
      "right context (Mikolov et al., 2013).\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 7\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 3921\n",
      "end_pos: 4883\n",
      "total_chunks: 25\n",
      "sentence_count: 5\n",
      "start_sentence_idx: 25\n",
      "end_sentence_idx: 29\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Pre-trained word embeddings\n",
      "are an integral part of modern NLP systems, of-\n",
      "fering significant improvements over embeddings\n",
      "learned from scratch (Turian et al., 2010). To pre-\n",
      "train word embedding vectors, left-to-right lan-\n",
      "guage modeling objectives have been used (Mnih\n",
      "and Hinton, 2009), as well as objectives to dis-\n",
      "criminate correct from incorrect words in left and\n",
      "right context (Mikolov et al., 2013). These approaches have been generalized to\n",
      "coarser granularities, such as sentence embed-\n",
      "dings (Kiros et al., 2015; Logeswaran and Lee,\n",
      "2018) or paragraph embeddings (Le and Mikolov,\n",
      "2014). To train sentence representations, prior\n",
      "work has used objectives to rank candidate next\n",
      "sentences (Jernite et al., 2017; Logeswaran and\n",
      "Lee, 2018), left-to-right generation of next sen-\n",
      "tence words given a representation of the previous\n",
      "sentence (Kiros et al., 2015), or denoising auto-\n",
      "encoder derived objectives (Hill et al., 2016).\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 8\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 4473\n",
      "end_pos: 5409\n",
      "total_chunks: 25\n",
      "sentence_count: 4\n",
      "start_sentence_idx: 28\n",
      "end_sentence_idx: 31\n",
      "\n",
      " -------------------- \n",
      "\n",
      "These approaches have been generalized to\n",
      "coarser granularities, such as sentence embed-\n",
      "dings (Kiros et al., 2015; Logeswaran and Lee,\n",
      "2018) or paragraph embeddings (Le and Mikolov,\n",
      "2014). To train sentence representations, prior\n",
      "work has used objectives to rank candidate next\n",
      "sentences (Jernite et al., 2017; Logeswaran and\n",
      "Lee, 2018), left-to-right generation of next sen-\n",
      "tence words given a representation of the previous\n",
      "sentence (Kiros et al., 2015), or denoising auto-\n",
      "encoder derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017,\n",
      "2018a) generalize traditional word embedding re-\n",
      "search along a different dimension. They extract\n",
      "context-sensitive features from a left-to-right and a\n",
      "right-to-left language model. The contextual rep-\n",
      "resentation of each token is the concatenation of\n",
      "the left-to-right and right-to-left representations.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 9\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 4883\n",
      "end_pos: 5761\n",
      "total_chunks: 25\n",
      "sentence_count: 5\n",
      "start_sentence_idx: 30\n",
      "end_sentence_idx: 34\n",
      "\n",
      " -------------------- \n",
      "\n",
      "They extract\n",
      "context-sensitive features from a left-to-right and a\n",
      "right-to-left language model. The contextual rep-\n",
      "resentation of each token is the concatenation of\n",
      "the left-to-right and right-to-left representations. When integrating contextual word embeddings\n",
      "with existing task-specific architectures, ELMo\n",
      "advances the state of the art for several major NLP\n",
      "benchmarks (Peters et al., 2018a) including ques-\n",
      "tion answering (Rajpurkar et al., 2016), sentiment\n",
      "analysis (Socher et al., 2013), and named entity\n",
      "recognition (Tjong Kim Sang and De Meulder,\n",
      "2003). (2016) proposed learning\n",
      "contextual representations through a task to pre-\n",
      "dict a single word from both left and right context\n",
      "using LSTMs. Similar to ELMo, their model is\n",
      "feature-based and not deeply bidirectional. Fedus\n",
      "et al. (2018) shows that the cloze task can be used\n",
      "to improve the robustness of text generation mod-\n",
      "els.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 10\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 5541\n",
      "end_pos: 6450\n",
      "total_chunks: 25\n",
      "sentence_count: 6\n",
      "start_sentence_idx: 33\n",
      "end_sentence_idx: 38\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Similar to ELMo, their model is\n",
      "feature-based and not deeply bidirectional. Fedus\n",
      "et al. (2018) shows that the cloze task can be used\n",
      "to improve the robustness of text generation mod-\n",
      "els. 2.2 Unsupervised Fine-tuning Approaches\n",
      "As with the feature-based approaches, the first\n",
      "works in this direction only pre-trained word em-\n",
      "bedding parameters from unlabeled text (Col-\n",
      "lobert and Weston, 2008). More recently, sentence or document encoders\n",
      "which produce contextual token representations\n",
      "have been pre-trained from unlabeled text and\n",
      "fine-tuned for a supervised downstream task (Dai\n",
      "and Le, 2015; Howard and Ruder, 2018; Radford\n",
      "et al., 2018). The advantage of these approaches\n",
      "is that few parameters need to be learned from\n",
      "scratch. At least partly due to this advantage,\n",
      "OpenAI GPT (Radford et al., 2018) achieved pre-\n",
      "viously state-of-the-art results on many sentence-\n",
      "level tasks from the GLUE benchmark (Wang\n",
      "et al., 2018a). Left-to-right language model-\n",
      "BERT BERT\n",
      "E[CLS] E\n",
      "1\n",
      "E\n",
      "[SEP]\n",
      "...\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 11\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 6261\n",
      "end_pos: 7255\n",
      "total_chunks: 25\n",
      "sentence_count: 7\n",
      "start_sentence_idx: 37\n",
      "end_sentence_idx: 43\n",
      "\n",
      " -------------------- \n",
      "\n",
      "At least partly due to this advantage,\n",
      "OpenAI GPT (Radford et al., 2018) achieved pre-\n",
      "viously state-of-the-art results on many sentence-\n",
      "level tasks from the GLUE benchmark (Wang\n",
      "et al., 2018a). Left-to-right language model-\n",
      "BERT BERT\n",
      "E[CLS] E\n",
      "1\n",
      "E\n",
      "[SEP]\n",
      "... TokM\n",
      "Question Paragraph\n",
      "Start/End Span\n",
      "BERT\n",
      "E[CLS] E\n",
      "1\n",
      "E TokM\n",
      "Masked Sentence A Masked Sentence B\n",
      "Pre-training Fine-Tuning\n",
      "NSP Mask LM Mask LM\n",
      "Unlabeled Sentence A and B Pair\n",
      "SQuAD\n",
      "Question Answer Pair\n",
      "NER MNLI\n",
      "Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec-\n",
      "tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\n",
      "models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\n",
      "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\n",
      "tions/answers).\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 12\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 6997\n",
      "end_pos: 8130\n",
      "total_chunks: 25\n",
      "sentence_count: 8\n",
      "start_sentence_idx: 42\n",
      "end_sentence_idx: 49\n",
      "\n",
      " -------------------- \n",
      "\n",
      "During fine-tuning, all parameters are fine-tuned. [CLS] is a special\n",
      "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\n",
      "tions/answers). ing and auto-encoder objectives have been used\n",
      "for pre-training such models (Howard and Ruder,\n",
      "2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 13\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 7936\n",
      "end_pos: 8272\n",
      "total_chunks: 25\n",
      "sentence_count: 3\n",
      "start_sentence_idx: 48\n",
      "end_sentence_idx: 50\n",
      "\n",
      " -------------------- \n",
      "\n",
      "[CLS] is a special\n",
      "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\n",
      "tions/answers). ing and auto-encoder objectives have been used\n",
      "for pre-training such models (Howard and Ruder,\n",
      "2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 14\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 7987\n",
      "end_pos: 8272\n",
      "total_chunks: 25\n",
      "sentence_count: 2\n",
      "start_sentence_idx: 49\n",
      "end_sentence_idx: 50\n",
      "\n",
      " -------------------- \n",
      "\n",
      "ing and auto-encoder objectives have been used\n",
      "for pre-training such models (Howard and Ruder,\n",
      "2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 15\n",
      "chunk_type: sliding_window\n",
      "section: introduction\n",
      "start_pos: 8130\n",
      "end_pos: 8272\n",
      "total_chunks: 25\n",
      "sentence_count: 1\n",
      "start_sentence_idx: 50\n",
      "end_sentence_idx: 50\n",
      "\n",
      " -------------------- \n",
      "\n",
      "model begins to outperform the LTR model\n",
      "almost immediately. C.2 Ablation for Different Masking\n",
      "Procedures In Section 3.1, we mention that BERT uses a\n",
      "mixed strategy for masking the target tokens when\n",
      "pre-training with the masked language model\n",
      "(MLM) objective. The following is an ablation\n",
      "study to evaluate the effect of different masking\n",
      "strategies. 200 400 600 800 1,000\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "Pre-trainingSteps(Thousands)\n",
      "M\n",
      "N\n",
      "LI\n",
      "D e\n",
      "v\n",
      "A c\n",
      "c\n",
      "ur\n",
      "a c\n",
      "y\n",
      "BERTBASE (MaskedLM)\n",
      "BERTBASE (Left-to-Right)\n",
      "Figure 5: Ablation over number of training steps. This\n",
      "shows the MNLI accuracy after fine-tuning, starting\n",
      "from model parameters that have been pre-trained for\n",
      "k steps. The x-axis is the value of k.\n",
      "Note that the purpose of the masking strategies\n",
      "is to reduce the mismatch between pre-training\n",
      "and fine-tuning, as the [MASK] symbol never ap-\n",
      "pears during the fine-tuning stage. We report the\n",
      "Dev results for both MNLI and NER.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 16\n",
      "chunk_type: sliding_window\n",
      "section: model\n",
      "start_pos: 0\n",
      "end_pos: 918\n",
      "total_chunks: 25\n",
      "sentence_count: 8\n",
      "start_sentence_idx: 0\n",
      "end_sentence_idx: 7\n",
      "\n",
      " -------------------- \n",
      "\n",
      "The x-axis is the value of k.\n",
      "Note that the purpose of the masking strategies\n",
      "is to reduce the mismatch between pre-training\n",
      "and fine-tuning, as the [MASK] symbol never ap-\n",
      "pears during the fine-tuning stage. We report the\n",
      "Dev results for both MNLI and NER. For NER,\n",
      "we report both fine-tuning and feature-based ap-\n",
      "proaches, as we expect the mismatch will be am-\n",
      "plified for the feature-based approach as the model\n",
      "will not have the chance to adjust the representa-\n",
      "tions. Masking Rates Dev Set Results\n",
      "MASK SAME RND MNLI NER\n",
      "Fine-tune Fine-tune Feature-based\n",
      "80% 10% 10% 84.2 95.4 94.9\n",
      "100% 0% 0% 84.3 94.9 94.0\n",
      "80% 0% 20% 84.1 95.2 94.6\n",
      "80% 20% 0% 84.4 95.2 94.7\n",
      "0% 20% 80% 83.7 94.8 94.6\n",
      "0% 0% 100% 83.6 94.9 94.6\n",
      "Table 8: Ablation over different masking strategies. The results are presented in Table 8.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 17\n",
      "chunk_type: sliding_window\n",
      "section: model\n",
      "start_pos: 661\n",
      "end_pos: 1469\n",
      "total_chunks: 25\n",
      "sentence_count: 5\n",
      "start_sentence_idx: 6\n",
      "end_sentence_idx: 10\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Masking Rates Dev Set Results\n",
      "MASK SAME RND MNLI NER\n",
      "Fine-tune Fine-tune Feature-based\n",
      "80% 10% 10% 84.2 95.4 94.9\n",
      "100% 0% 0% 84.3 94.9 94.0\n",
      "80% 0% 20% 84.1 95.2 94.6\n",
      "80% 20% 0% 84.4 95.2 94.7\n",
      "0% 20% 80% 83.7 94.8 94.6\n",
      "0% 0% 100% 83.6 94.9 94.6\n",
      "Table 8: Ablation over different masking strategies. The results are presented in Table 8. In the table,\n",
      "MASK means that we replace the target token with\n",
      "the [MASK] symbol for MLM; SAME means that\n",
      "we keep the target token as is; RND means that\n",
      "we replace the target token with another random\n",
      "token. The numbers in the left part of the table repre-\n",
      "sent the probabilities of the specific strategies used\n",
      "during MLM pre-training (BERT uses 80%, 10%,\n",
      "10%). The right part of the paper represents the\n",
      "Dev set results. For the feature-based approach,\n",
      "we concatenate the last 4 layers of BERT as the\n",
      "features, which was shown to be the best approach\n",
      "in Section 5.3.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 18\n",
      "chunk_type: sliding_window\n",
      "section: model\n",
      "start_pos: 1135\n",
      "end_pos: 2039\n",
      "total_chunks: 25\n",
      "sentence_count: 6\n",
      "start_sentence_idx: 9\n",
      "end_sentence_idx: 14\n",
      "\n",
      " -------------------- \n",
      "\n",
      "The right part of the paper represents the\n",
      "Dev set results. For the feature-based approach,\n",
      "we concatenate the last 4 layers of BERT as the\n",
      "features, which was shown to be the best approach\n",
      "in Section 5.3. From the table it can be seen that fine-tuning is\n",
      "surprisingly robust to different masking strategies. However, as expected, using only the MASK strat-\n",
      "egy was problematic when applying the feature-\n",
      "based approach to NER. Interestingly, using only\n",
      "the RND strategy performs much worse than our\n",
      "strategy as well.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 19\n",
      "chunk_type: sliding_window\n",
      "section: model\n",
      "start_pos: 1833\n",
      "end_pos: 2350\n",
      "total_chunks: 25\n",
      "sentence_count: 5\n",
      "start_sentence_idx: 13\n",
      "end_sentence_idx: 17\n",
      "\n",
      " -------------------- \n",
      "\n",
      "However, as expected, using only the MASK strat-\n",
      "egy was problematic when applying the feature-\n",
      "based approach to NER. Interestingly, using only\n",
      "the RND strategy performs much worse than our\n",
      "strategy as well.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 20\n",
      "chunk_type: sliding_window\n",
      "section: model\n",
      "start_pos: 2142\n",
      "end_pos: 2350\n",
      "total_chunks: 25\n",
      "sentence_count: 2\n",
      "start_sentence_idx: 16\n",
      "end_sentence_idx: 17\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Interestingly, using only\n",
      "the RND strategy performs much worse than our\n",
      "strategy as well.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 21\n",
      "chunk_type: sliding_window\n",
      "section: model\n",
      "start_pos: 2261\n",
      "end_pos: 2350\n",
      "total_chunks: 25\n",
      "sentence_count: 1\n",
      "start_sentence_idx: 17\n",
      "end_sentence_idx: 17\n",
      "\n",
      " -------------------- \n",
      "\n",
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT, which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be fine-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "specific architecture modifications.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute impro\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 22\n",
      "chunk_type: section\n",
      "section: abstract\n",
      "start_pos: 213\n",
      "end_pos: 1094\n",
      "total_chunks: 25\n",
      "section_length: 881\n",
      "is_hierarchical: True\n",
      "\n",
      " -------------------- \n",
      "\n",
      "1 Introduction\n",
      "Language model pre-training has been shown to\n",
      "be effective for improving many natural language\n",
      "processing tasks (Dai and Le, 2015; Peters et al.,\n",
      "2018a; Radford et al., 2018; Howard and Ruder,\n",
      "2018). These include sentence-level tasks such as\n",
      "natural language inference (Bowman et al., 2015;\n",
      "Williams et al., 2018) and paraphrasing (Dolan\n",
      "and Brockett, 2005), which aim to predict the re-\n",
      "lationships between sentences by analyzing them\n",
      "holistically, as well as token-level tasks such as\n",
      "named entity recognition and question answering,\n",
      "where models are required to produce fine-grained\n",
      "output at the token level (Tjong Kim Sang and\n",
      "De Meulder, 2003; Rajpurkar et al., 2016).\n",
      "There are two existing strategies for apply-\n",
      "ing pre-trained language representations to down-\n",
      "stream tasks: feature-based and fine-tuning. The\n",
      "feature-based approach, such as ELMo (Peters\n",
      "et al., 2018a), uses task-specific architectures that\n",
      "include the pre-trained representations as addi-\n",
      "tional features. The fine-tuning approach, such as\n",
      "the Generative Pre-trained Transformer (OpenAI\n",
      "GPT) (Radford et al., 2018), introduces minimal\n",
      "task-specific parameters, and is trained on the\n",
      "downstream tasks by simply fine-tuning all pre-\n",
      "trained parameters. The two approaches share the\n",
      "same objective function during pre-training, where\n",
      "they use unidirectional language models to learn\n",
      "general language representations.\n",
      "We argue that current techniques restrict the\n",
      "power of the pre-trained representations, espe-\n",
      "cially for the fine-tuning approaches. The ma-\n",
      "jor limitation is that standard language models are\n",
      "unidirectional, and this limits the choice of archi-\n",
      "tectures that can be used during pre-training. For\n",
      "example, in OpenAI GPT, the authors use a left-to-\n",
      "right architecture, where every token can only at-\n",
      "tend to previous tokens in the self-attention layers\n",
      "of the Transformer (Vaswani et al., 2017). Such re-\n",
      "strictions are sub-optimal for sentence-level tasks,\n",
      "and could be very harmful when applying fine-\n",
      "tuning based approaches to token-level tasks such\n",
      "as question answering, where it is crucial to incor-\n",
      "porate context from both directions.\n",
      "In this paper, we improve the fine-tuning based\n",
      "approaches by proposing BERT: Bidirectional\n",
      "Encoder Representations from Transformers.\n",
      "BERT alleviates the previously mentioned unidi-\n",
      "rectionality constraint by using a “masked lan-\n",
      "guage model” (MLM) pre-training objective, in-\n",
      "spired by the Cloze task (Taylor, 1953). The\n",
      "masked language model randomly masks some of\n",
      "the tokens from the input, and the objective is to\n",
      "predict the original vocabulary id of the masked\n",
      "a\n",
      "r\n",
      "X\n",
      "i\n",
      "v\n",
      ":\n",
      "1\n",
      "8\n",
      "1\n",
      "0\n",
      ".\n",
      "0\n",
      "4\n",
      "8\n",
      "0\n",
      "5\n",
      "v\n",
      "2\n",
      "[\n",
      "c\n",
      "s\n",
      ".\n",
      "C\n",
      "L\n",
      "]\n",
      "2\n",
      "4\n",
      "M\n",
      "a\n",
      "y\n",
      "2\n",
      "0\n",
      "1\n",
      "9\n",
      "word based only on its context. Unlike left-to-\n",
      "right language model pre-training, the MLM ob-\n",
      "jective enables the representation to fuse the left\n",
      "and the right context, which allows us to pre-\n",
      "train a deep bidirectional Transformer. In addi-\n",
      "tion to the masked language model, we also use\n",
      "a “next sentence prediction” task that jointly pre-\n",
      "trains text-pair representations. The contributions\n",
      "of our paper are as follows:\n",
      "• We demonstrate the importance of bidirectional\n",
      "pre-training for language representations. Un-\n",
      "like Radford et al. (2018), which uses unidirec-\n",
      "tional language models for pre-training, BERT\n",
      "uses masked language models to enable pre-\n",
      "trained deep bidirectional representations. This\n",
      "is also in contrast to Peters et al. (2018a), which\n",
      "uses a shallow concatenation of independently\n",
      "trained left-to-right and right-to-left LMs.\n",
      "• We show that pre-trained representations reduce\n",
      "the need for many heavily-engineered task-\n",
      "specific architectures. BERT is the first fine-\n",
      "tuning based representation model that achieves\n",
      "state-of-the-art performance on a large suite\n",
      "of sentence-level and token-level tasks, outper-\n",
      "forming many task-specific architectures.\n",
      "• BERT advances the state of the art for eleven\n",
      "NLP tasks. The code and pre-trained mod-\n",
      "els are available at https://github.com/\n",
      "google-research/bert.\n",
      "2 Related Work\n",
      "There is a long history of pre-training general lan-\n",
      "guage representations, and we briefly review the\n",
      "most widely-used approaches in this section.\n",
      "2.1 Unsupervised Feature-based Approaches\n",
      "Learning widely applicable representations of\n",
      "words has been an active area of research for\n",
      "decades, including non-neural (Brown et al., 1992;\n",
      "Ando and Zhang, 2005; Blitzer et al., 2006) and\n",
      "neural (Mikolov et al., 2013; Pennington et al.,\n",
      "2014) methods. Pre-trained word embeddings\n",
      "are an integral part of modern NLP systems, of-\n",
      "fering significant improvements over embeddings\n",
      "learned from scratch (Turian et al., 2010). To pre-\n",
      "train word embedding vectors, left-to-right lan-\n",
      "guage modeling objectives have been used (Mnih\n",
      "and Hinton, 2009), as well as objectives to dis-\n",
      "criminate correct from incorrect words in left and\n",
      "right context (Mikolov et al., 2013).\n",
      "These approaches have been generalized to\n",
      "coarser granularities, such as sentence embed-\n",
      "dings (Kiros et al., 2015; Logeswaran and Lee,\n",
      "2018) or paragraph embeddings (Le and Mikolov,\n",
      "2014). To train sentence representations, prior\n",
      "work has used objectives to rank candidate next\n",
      "sentences (Jernite et al., 2017; Logeswaran and\n",
      "Lee, 2018), left-to-right generation of next sen-\n",
      "tence words given a representation of the previous\n",
      "sentence (Kiros et al., 2015), or denoising auto-\n",
      "encoder derived objectives (Hill et al., 2016).\n",
      "ELMo and its predecessor (Peters et al., 2017,\n",
      "2018a) generalize traditional word embedding re-\n",
      "search along a different dimension. They extract\n",
      "context-sensitive features from a left-to-right and a\n",
      "right-to-left language model. The contextual rep-\n",
      "resentation of each token is the concatenation of\n",
      "the left-to-right and right-to-left representations.\n",
      "When integrating contextual word embeddings\n",
      "with existing task-specific architectures, ELMo\n",
      "advances the state of the art for several major NLP\n",
      "benchmarks (Peters et al., 2018a) including ques-\n",
      "tion answering (Rajpurkar et al., 2016), sentiment\n",
      "analysis (Socher et al., 2013), and named entity\n",
      "recognition (Tjong Kim Sang and De Meulder,\n",
      "2003). Melamud et al. (2016) proposed learning\n",
      "contextual representations through a task to pre-\n",
      "dict a single word from both left and right context\n",
      "using LSTMs. Similar to ELMo, their model is\n",
      "feature-based and not deeply bidirectional. Fedus\n",
      "et al. (2018) shows that the cloze task can be used\n",
      "to improve the robustness of text generation mod-\n",
      "els.\n",
      "2.2 Unsupervised Fine-tuning Approaches\n",
      "As with the feature-based approaches, the first\n",
      "works in this direction only pre-trained word em-\n",
      "bedding parameters from unlabeled text (Col-\n",
      "lobert and Weston, 2008).\n",
      "More recently, sentence or document encoders\n",
      "which produce contextual token representations\n",
      "have been pre-trained from unlabeled text and\n",
      "fine-tuned for a supervised downstream task (Dai\n",
      "and Le, 2015; Howard and Ruder, 2018; Radford\n",
      "et al., 2018). The advantage of these approaches\n",
      "is that few parameters need to be learned from\n",
      "scratch. At least partly due to this advantage,\n",
      "OpenAI GPT (Radford et al., 2018) achieved pre-\n",
      "viously state-of-the-art results on many sentence-\n",
      "level tasks from the GLUE benchmark (Wang\n",
      "et al., 2018a). Left-to-right language model-\n",
      "BERT BERT\n",
      "E[CLS] E\n",
      "1\n",
      "E\n",
      "[SEP]\n",
      "... E\n",
      "N\n",
      "E\n",
      "1\n",
      "’ ... E\n",
      "M\n",
      "’\n",
      "C T\n",
      "1\n",
      "T\n",
      "[SEP]\n",
      "... T\n",
      "N\n",
      "T\n",
      "1\n",
      "’ ... T\n",
      "M\n",
      "’\n",
      "[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\n",
      "Question Paragraph\n",
      "Start/End Span\n",
      "BERT\n",
      "E[CLS] E\n",
      "1\n",
      "E\n",
      "[SEP]\n",
      "... E\n",
      "N\n",
      "E\n",
      "1\n",
      "’ ... E\n",
      "M\n",
      "’\n",
      "C T\n",
      "1\n",
      "T\n",
      "[SEP]\n",
      "... T\n",
      "N\n",
      "T\n",
      "1\n",
      "’ ... T\n",
      "M\n",
      "’\n",
      "[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\n",
      "Masked Sentence A Masked Sentence B\n",
      "Pre-training Fine-Tuning\n",
      "NSP Mask LM Mask LM\n",
      "Unlabeled Sentence A and B Pair\n",
      "SQuAD\n",
      "Question Answer Pair\n",
      "NER MNLI\n",
      "Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec-\n",
      "tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\n",
      "models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\n",
      "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\n",
      "tions/answers).\n",
      "ing and auto-encoder objectives have been used\n",
      "for pre-training such models (Howard and Ruder,\n",
      "2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "2.3 T\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 23\n",
      "chunk_type: section\n",
      "section: introduction\n",
      "start_pos: 1307\n",
      "end_pos: 9584\n",
      "total_chunks: 25\n",
      "section_length: 8277\n",
      "is_hierarchical: True\n",
      "\n",
      " -------------------- \n",
      "\n",
      "model begins to outperform the LTR model\n",
      "almost immediately.\n",
      "C.2 Ablation for Different Masking\n",
      "Procedures\n",
      "In Section 3.1, we mention that BERT uses a\n",
      "mixed strategy for masking the target tokens when\n",
      "pre-training with the masked language model\n",
      "(MLM) objective. The following is an ablation\n",
      "study to evaluate the effect of different masking\n",
      "strategies.\n",
      "200 400 600 800 1,000\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "Pre-trainingSteps(Thousands)\n",
      "M\n",
      "N\n",
      "LI\n",
      "D e\n",
      "v\n",
      "A c\n",
      "c\n",
      "ur\n",
      "a c\n",
      "y\n",
      "BERTBASE (MaskedLM)\n",
      "BERTBASE (Left-to-Right)\n",
      "Figure 5: Ablation over number of training steps. This\n",
      "shows the MNLI accuracy after fine-tuning, starting\n",
      "from model parameters that have been pre-trained for\n",
      "k steps. The x-axis is the value of k.\n",
      "Note that the purpose of the masking strategies\n",
      "is to reduce the mismatch between pre-training\n",
      "and fine-tuning, as the [MASK] symbol never ap-\n",
      "pears during the fine-tuning stage. We report the\n",
      "Dev results for both MNLI and NER. For NER,\n",
      "we report both fine-tuning and feature-based ap-\n",
      "proaches, as we expect the mismatch will be am-\n",
      "plified for the feature-based approach as the model\n",
      "will not have the chance to adjust the representa-\n",
      "tions.\n",
      "Masking Rates Dev Set Results\n",
      "MASK SAME RND MNLI NER\n",
      "Fine-tune Fine-tune Feature-based\n",
      "80% 10% 10% 84.2 95.4 94.9\n",
      "100% 0% 0% 84.3 94.9 94.0\n",
      "80% 0% 20% 84.1 95.2 94.6\n",
      "80% 20% 0% 84.4 95.2 94.7\n",
      "0% 20% 80% 83.7 94.8 94.6\n",
      "0% 0% 100% 83.6 94.9 94.6\n",
      "Table 8: Ablation over different masking strategies.\n",
      "The results are presented in Table 8. In the table,\n",
      "MASK means that we replace the target token with\n",
      "the [MASK] symbol for MLM; SAME means that\n",
      "we keep the target token as is; RND means that\n",
      "we replace the target token with another random\n",
      "token.\n",
      "The numbers in the left part of the table repre-\n",
      "sent the probabilities of the specific strategies used\n",
      "during MLM pre-training (BERT uses 80%, 10%,\n",
      "10%). The right part of the paper represents the\n",
      "Dev set results. For the feature-based approach,\n",
      "we concatenate the last 4 layers of BERT as the\n",
      "features, which was shown to be the best approach\n",
      "in Section 5.3.\n",
      "From the table it can be seen that fine-tuning is\n",
      "surprisingly robust to different masking strategies.\n",
      "However, as expected, using only the MASK strat-\n",
      "egy was problematic when applying the feature-\n",
      "based approach to NER. Interestingly, using only\n",
      "the RND strategy performs much worse than our\n",
      "strategy as well.\n",
      "\n",
      "\n",
      "source: BERT_pre-training_deep_bidirectional_transformers.pdf\n",
      "chunk_id: 24\n",
      "chunk_type: section\n",
      "section: model\n",
      "start_pos: 62022\n",
      "end_pos: 64372\n",
      "total_chunks: 25\n",
      "section_length: 2350\n",
      "is_hierarchical: True\n",
      "\n",
      " -------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "        for segment in docs[i]['content'].split('\\n'):\n",
    "            print(segment)\n",
    "        print('\\n')\n",
    "        for key, value in docs[i]['metadata'].items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print('\\n','-'*20,'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academic_rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
